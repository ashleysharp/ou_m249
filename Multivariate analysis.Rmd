---
title: "Multivariate analysis"
output: html_document
date: "2025-07-05"
---

```{r message=FALSE}
library(tidyverse)
library(janitor)
library(haven)
library(GGally)
library(janitor)
library(gt)

data(iris)
```

# Data visualisation

## Scatterplots

### Housing

```{r}
housing <- read_sav("data/Book 3/housing.sav")
```

```{r}
ggplot(
  housing,
  aes(
    income,
    houseprice,
    fill  = factor(region),
    shape = factor(region)
  )
) +
  geom_point(
    size   = 2,
    colour = "black",
    alpha  = 0.5
  ) +
  scale_shape_manual(values = rep(c(21, 22, 23), 3)) +
  theme_minimal()
```

### Crime

```{r}
crimeloss1 <- read_sav("data/Book 3/crimeloss1.sav")
```

```{r}
ggplot(
  crimeloss1,
  aes(
    totexp,
    totloss,
    shape = region
  )
) +
  geom_point(
    size   = 2,
    colour = "black",
    alpha  = 0.7
  ) +
  theme_minimal()
```

## Matrix scatterplots

### Iris

```{r message=FALSE}
ggpairs(
  iris,
  columns = 1:4,
  aes(color = Species, alpha = 0.7)
) + 
  theme_minimal()
```

### Local education authorities

```{r}
lea <- read_sav("data/Book 3/lea.sav")
```

```{r}
ggpairs(
  lea,
  columns = 2:4,
) + 
  theme_minimal()
```

```{r}
crimeloss2 <- read_sav("data/Book 3/crimeloss2.sav")
```

```{r message=FALSE}
ggpairs(
  crimeloss2,
  columns = 2:5,
) + 
  theme_minimal()
```

## Profile plots

### Iris

```{r}
iris_long <- iris |>
  mutate(id = row_number()) |>
  pivot_longer(
    cols = Sepal.Length:Petal.Width,
    names_to = "variable",
    values_to = "value"
  ) |>
  group_by(variable) |>
  mutate(variable_mean = mean(value)) |>
  ungroup() |>
  mutate(
    variable = reorder(variable, -variable_mean)
  )

ggplot(
  iris_long,
  aes(x = variable, y = value, colour = Species, group = id)
) +
  geom_line(alpha = 0.25, linewidth = 0.6) +
  theme_minimal() +
  labs(
    x = NULL,
    y = "Measurement",
    title = "Iris Profile Plot"
  )
```

### Echinacea

```{r}
echinacea <- read_sav("data/Book 3/echinacea.sav")
```

```{r}
echinacea_long <- echinacea |>
  pivot_longer(
    cols = lCu:lCa,
    names_to = "variable",
    values_to = "value"
  ) |>
  group_by(variable) |>
  mutate(variable_mean = mean(value)) |>
  ungroup() |>
  mutate(
    variable = reorder(variable, variable_mean)
  )

ggplot(
  echinacea_long,
  aes(x = variable, y = value, col = sample, lty = sample, group = sample)
) +
  geom_line(alpha = 0.5, linewidth = 1) +
  theme_minimal() +
  labs(
    x = NULL,
    y = "Measurement",
    title = "Echinacea Profile Plot"
  )
```

### Iberian hams

```{r}
hams <- read_sav("data/Book 3/hams.sav")
```

```{r}
hams_long <- hams |>
  pivot_longer(
    cols = s1:s15,
    names_to = "variable",
    values_to = "value"
  ) |>
  mutate(
    variable = factor(variable, levels = paste0("s", 1:15)),
    setting = factor(setting)
  )

ggplot(
  hams_long,
  aes(x = variable, y = value, col = setting, group = setting)
) +
  geom_line(alpha = 0.5, linewidth = 0.8) +
  theme_minimal() +
  labs(
    x = NULL,
    y = "Measurement",
    title = "Iberian Hams Profile Plot"
  )
```

# Numerical summaries and standardisation

## Mean vectors

### Housing

```{r}
housing |>
  summarise(
    across(
      c(houseprice, income),
      \(x) mean(x, na.rm = TRUE) # Lambda function x |-> mean(x)
    )
  )
```

```{r}
housing |>
  select(houseprice, income) |>
  colMeans(na.rm = TRUE)
```

```{r}
housing |>
  group_by(region) |> 
  summarise(
    across(
      c(houseprice, income),
      \(x) mean(x, na.rm = TRUE) # Lambda function x |-> mean(x)
    )
  )
```

## Standardising variables

### Housing

```{r}
housing_Z <- housing |>
  mutate(
    across(
      c(houseprice, income),
      \(x) as.numeric(scale(x)), #scale() outputs a matrix, make numeric
      .names = "Z{.col}"
      )
    )
housing_Z
```

```{r}
# housing_Z <- housing
# Z_housing_[, c("houseprice", "income")] <- scale(housing[, c("houseprice", "income")])
# housing_Z
```

```{r}
housing_Z |>
  summarise(
    across(
      c(Zhouseprice, Zincome),
      \(x) mean(x, na.rm = TRUE) # Lambda function x |-> mean(x)
    )
  )
```

## Covariance and correlation matrices

### Maths ability

```{r}
mathsability <- read_sav("data/Book 3/mathsability.sav")
```

```{r}
cov(mathsability |> select(age:algebra))
```

```{r}
cor(mathsability |> select(age:algebra))
```

### Crime

```{r}
crimeloss2 <- read_sav("data/Book 3/crimeloss2.sav")
```

```{r}
crimeloss2 |> summarise(
  across(
    proploss:localexp,
    \(x) mean(x, na.rm = TRUE)
    )
  )
```

```{r}
cov(crimeloss2 |> select(proploss:localexp))
```

```{r}
cor(crimeloss2 |> select(proploss:localexp))
```

# Principle Component analysis

## Iris

### base

```{r}
X <- as.matrix(iris[, 1:4])

# --- choose: covariance PCA (center only) or correlation PCA (center + scale)
# Xc <- scale(X, center = TRUE, scale = FALSE)  # covariance version
Z  <- scale(X, center = TRUE, scale = TRUE) # correlation version

# S <- cov(Xc)               # same as t(Xc) %*% Xc / (nrow(Xc) - 1)
S <- cov(Z)

eig <- eigen(S)

values  <- eig$values      # eigenvalues (variances of PCs)
vectors <- eig$vectors     # eigenvectors (loadings)

# PC scores
scores <- Z %*% vectors

# proportion variance explained
pve <- values / sum(values)

values
pve
head(scores)

pc1 <- Z %*% vectors[,1]
pc2 <- Z %*% vectors[,1:2]
```

### prcomp()

```{r}
X <- iris[, 1:4]

p <- prcomp(X, center = TRUE, scale. = TRUE)  # correlation PCA
summary(p)

# loadings (eigenvectors)
p$rotation

# scores
head(p$x)

# eigenvalues of correlation PCA:
p$sdev^2

# proportion variance explained
p$sdev^2 / sum(p$sdev^2)
```

## Standardised data

### Maths ability

```{r}
X <- mathsability[, 3:6]
p <- prcomp(X, center = TRUE, scale. = TRUE)  # correlation PCA
```

```{r}
summary(p)
```

```{r}
# eigenvalues of correlation PCA:
p$sdev
p$sdev^2
```

```{r}
# loadings (eigenvectors). Note: sum of squared loadings = 1
p$rotation
```

```{r}
sum(p$rotation[,1]^2)
```

```{r}
# loadings (eigenvectors). Note: SGSS convention (the sum of squared loadings for component k equals the variance of that component). Negative is an artifact of axis choice.

sweep(-p$rotation, 2, p$sdev, `*`)
```

### Wells

```{r}
wells <- read_sav("data/Book 3/wells.sav")
```

```{r}
wells |>
  summarise(
    across(
      2:26,
      c(
        mean = \(x) mean(x, na.rm = TRUE),
        sd   = \(x) sd(x, na.rm = TRUE)
      )
    )
  )
```

```{r}
X <- wells[, 2:26]
p <- prcomp(X, center = TRUE, scale. = TRUE)  # correlation PCA
```

```{r}
eig <- p$sdev^2 # eigenvalues
keep <- eig[eig > 1] # Kaiser's criterion
keep
```

```{r}
pve <- eig / sum(eig) # proportion variance explained
cpve <- cumsum(pve) # cumulative
pve[1:5]
cpve[1:5]
```

```{r}
p$rotation[, 1:2]
```

## Unstandardised data

### Local education authorities

```{r}
X <- lea[,2:4]
p <- prcomp(X, center = TRUE, scale. = FALSE)
```

```{r}
summary(p)
```

```{r}
eig <- p$sdev^2
keep <- eig[eig > mean(eig)] # Kaiser's criterion, components with variance greater than the average
eig
keep
```

```{r}
tve <- sum(eig)
pve <- keep / tve
tve
pve
```

### Maths ability

```{r}
mathsability2 <- read_sav("data/Book 3/mathsability2.sav")
```

#### Standardised

```{r}
X <- mathsability2 |> select(A:I)
p <- prcomp(X, center = TRUE, scale. = TRUE)
```

```{r}
summary(p)
```

```{r}
eig <- p$sdev^2
keep <- eig[eig > 1]
eig
keep
```

```{r}
-p$rotation[,1:2]
```

#### Unstandardised

```{r}
X <- mathsability2 |> select(A:I)
p <- prcomp(X, center = TRUE, scale. = FALSE)
```

```{r}
summary(p)
```

```{r}
eig <- p$sdev^2
keep <- eig[eig > mean(eig)]
eig
keep
```

Variables with largest variances also have largest loadings (PCA chases variance)

```{r}
mathsability2 |> summarise(
  across(
    A:I,
    \(x) var(x, na.rm = )
  )
) |> 
  pivot_longer(everything())

-p$rotation[,1:3]
```

# Extracting and plotting principle components

## Scree plot

```{r}
X <- mathsability2 |> select(A:I)
p <- prcomp(X, center = TRUE, scale. = TRUE)
eig <- p$sdev^2
```

```{r}
ggplot(
  tibble(n = seq_along(eig), eig = eig),
  aes(x = n, y = eig)
) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = seq_along(eig)) +
  theme_minimal()
```

```{r}
-p$rotation[, 1:3]
```

## Scatterplot

```{r}
as_tibble(-p$x[,1:3]) |> ggpairs() + 
  theme_minimal()
```

```{r}
mathsability2_aug <- mathsability2 |>
  bind_cols(
    as_tibble(p$x[, 1:2]) |>
      rename(PC1 = 1, PC2 = 2)
  )
```

```{r}
ggplot(mathsability2_aug, aes(x = PC1, y = PC2, col = form)) + 
  geom_point() + 
  theme_minimal()
```

# Linear discriminant analysis

## Iris

### Variance
#### Pooled within group variance

**Sum of squares form**
$$
s_W^2
=
\frac{
\sum_{g=1}^G \sum_{i \in g}
\left(x_{ig} - \bar{x}_g\right)^2
}{
\sum_{g=1}^G (n_g - 1)
}
$$

**Variance-weighted form**
$$
s_W^2
=
\frac{\sum_{g=1}^G (n_g - 1)\, s_g^2}
{\sum_{g=1}^G (n_g - 1)}
$$
**Proof of equivalence**
$$
\begin{aligned}
\bar{x}_g &= \frac{1}{n_g}\sum_{i \in g} x_{ig} \\
s_g^2 &= \frac{1}{n_g - 1}
\sum_{i \in g}
\left(x_{ig} - \bar{x}_g\right)^2 \\
(n_g - 1)s_g^2 &=
\sum_{i \in g}
\left(x_{ig} - \bar{x}_g\right)^2
\end{aligned}
$$

```{r}
iris_groups <- iris |> 
  pivot_longer(1:4, names_to = "variable", values_to = "value") |> 
  group_by(Species, variable) |>
  summarise(n = n(),
            xbar = mean(value),
            sse = sum((value - xbar)^2),
            s2 = sse / (n-1),
            .groups = "drop") |> 
  select(variable, everything()) |> 
  arrange(variable)

iris_groups
```


```{r}
iris_w <- iris_groups |>
  group_by(variable) |>
  summarise(
    ssw = sum(sse),
    df_w = sum(n - 1),
    msw = ssw / df_w, # pooled from sse
    # msw  = sum(s2*(n - 1)) / df_w, # pooled from s2
    .groups = "drop"
  )
iris_w
```

#### Pooled between-group variance

**Sum of squares form**
$$

\mathrm{SS}_B
=
\sum_{g=1}^{G}
n_g
\left(
\bar{x}_g - \bar{x}
\right)^2,
\qquad
s_B^2
=
\frac{\mathrm{SS}_B}{G - 1}
$$

**Group means form**
$$
s_B^2
=
\frac{1}{G - 1}
\sum_{g=1}^{G}
n_g
\left(
\bar{x}_g - \bar{x}
\right)^2
$$

```{r}
grand_means <- iris |>
  pivot_longer(1:4, names_to = "variable", values_to = "value") |>
  group_by(variable) |>
  summarise(grand_mean = mean(value), .groups = "drop")

iris_b <- iris_groups |>
  left_join(grand_means, by = "variable") |>
  group_by(variable) |>
  summarise(
    ssb = sum(n * (xbar - grand_mean)^2),
    df_b = n() - 1,
    msb = ssb / df_b,
    .groups = "drop"
  )
iris_b
```

#### Summary
```{r}
str_c(iris_b$variable,
      ", msb = ", round(iris_b$msb, 2), 
      ", msw = ", round(iris_w$msw, 2),
      ", msb/msw = ", round(iris_b$msb/iris_w$msw, 2),
      ", ss_t = ", round(iris_b$ssb + iris_w$ssw, 2))
```

**Note: total sum of squares = ssb + ssw**
```{r}
iris |> 
  pivot_longer(1:4, names_to = "variable", values_to = "value") |> 
  group_by(variable) |>
  summarise(
    sse = round(
    sum((value - mean(value))^2), 
    2),
            .groups = "drop")
```
####aov()
```{r}
summary(aov(Petal.Length ~ Species, data = iris))
summary(aov(Petal.Width ~ Species, data = iris))
summary(aov(Sepal.Length ~ Species, data = iris))
summary(aov(Sepal.Width ~ Species, data = iris))
```
### Covariance

#### Covariance matrix (centred variables)
```{r}
iris_centred <- scale(iris[, 1:4], center = TRUE, scale = FALSE)

t(iris_centred) %*% iris_centred / (nrow(iris) - 1)

# Equivalent:
# crossprod(iris_centred) / (nrow(iris) - 1)
# cov(iris[, 1:4])
```
#### Correlation matrix (standardised variables)
```{r}
iris_standardised <- scale(iris[, 1:4], center = TRUE, scale = TRUE)

t(iris_standardised) %*% iris_standardised / (nrow(iris) - 1) 

# Equivalent
# crossprod(iris_standardised) / (nrow(iris) - 1) 
# cor(iris[, 1:4])
```
#### Pooled within group covariance
```{r}
X <- as.matrix(iris[, 1:4])   # numeric variables
g <- iris$Species              # grouping factor

p <- ncol(X)
n <- nrow(X)
groups <- levels(g)
G <- length(groups)
```

```{r}
SW <- matrix(0, p, p)

for (grp in groups) {
  Xg <- X[g == grp, ]
  Xg_centered <- scale(Xg, center = TRUE, scale = FALSE)
  SW <- SW + t(Xg_centered) %*% Xg_centered
}
```

```{r}
SW_cov <- SW / (n - G)
SW_cov
```

#### Pooled between group covariance
```{r}
grand_mean <- colMeans(X)
SB <- matrix(0, p, p)

for (grp in groups) {
  Xg <- X[g == grp, ]
  ng <- nrow(Xg)
  mean_g <- colMeans(Xg)
  
  diff <- matrix(mean_g - grand_mean, ncol = 1)
  SB <- SB + ng * (diff %*% t(diff))
}
```

```{r}
SB_cov <- SB / (G - 1)
SB_cov
```

```{r}
Xc <- scale(X, center = TRUE, scale = FALSE)
ST <- t(Xc) %*% Xc

all.equal(ST, SW + SB)
```

#### manova()
```{r}
fit <- manova(cbind(
  Sepal.Length,
  Sepal.Width,
  Petal.Length,
  Petal.Width
) ~ Species, data = iris)

summary(fit)
```

### lda()

**Note: Unstandardised by default**
```{r}
fit <- MASS::lda(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
           data = iris)
fit
```

```{r}
# Predict
pred <- predict(fit, newdata = iris) #Don't need to specify newdata = iris, original data used by default
names(pred)
# "class" "posterior" "x"  (x = LD scores)
```

```{r}
# Confusion matrix (training set)
table(Truth = iris$Species, Pred = pred$class)
```

```{r}
# Accuracy
mean(pred$class == iris$Species)
```

```{r}
# LD scores
head(pred$x)   # columns LD1, LD2 (since 3 classes -> at most 2 LDs)
```

```{r}
lda_df <- data.frame(
  LD1 = pred$x[, 1],
  LD2 = pred$x[, 2],
  Species = iris$Species
)

ggplot(lda_df, aes(x = LD1, y = LD2, colour = Species)) +
  geom_point(size = 2, alpha = 0.8) +
  labs(
    x = "LD1",
    y = "LD2",
    colour = "Species"
  ) +
  theme_minimal()
```

## Obtaining a discriminant function
### Banknotes

```{r}
banknotes <- read_sav("data/Book 3/banknotes.sav") |>
  mutate(
    type = factor(
      type,
      levels = c(1, 2),
      labels = c("Genuine", "Counterfeit")
    )
  )
```

#### Standardised

```{r}
# ============================================================
# Recreate SPSS "Standardized Canonical Discriminant Function Coefficients"
# (Banknotes example) using an explicitly constructed pooled
# within-group covariance matrix
# ============================================================

# Rationale:
# ----------
# In Canonical Discriminant Analysis, SPSS reports
# "Standardized Canonical Discriminant Function Coefficients".
#
# These are NOT obtained by globally standardising variables.
# Instead, coefficients are scaled using the POOLED within-group
# standard deviations, derived from the pooled within-group
# covariance matrix S_W.
#
# If b are the unstandardised canonical coefficients from LDA,
# and sW_j are the pooled within-group SDs, then SPSS reports:
#
#   b*_j = b_j × sW_j
#
# The sign of the discriminant function is arbitrary and may
# differ between R and SPSS.

# ------------------------------------------------------------
# 1) Build X (numeric data matrix) and g (grouping factor)
# ------------------------------------------------------------

X <- as.matrix(banknotes[, c("length", "lwidth", "rwidth",
                             "bottom", "top", "diagonal")])

g <- as.factor(banknotes$type)

p <- ncol(X)
n <- nrow(X)
groups <- levels(g)
G <- length(groups)

# ------------------------------------------------------------
# 2) Construct pooled within-group SSCP matrix
#    SW = Σ_g (X_g − x̄_g)'(X_g − x̄_g)
# ------------------------------------------------------------

SW <- matrix(0, p, p)

for (grp in groups) {
  Xg <- X[g == grp, , drop = FALSE]
  Xg_centered <- scale(Xg, center = TRUE, scale = FALSE)
  SW <- SW + t(Xg_centered) %*% Xg_centered
}

# ------------------------------------------------------------
# 3) Convert to pooled within-group covariance matrix
#    S_W = SW / (n − G)
# ------------------------------------------------------------

SW_cov <- SW / (n - G)

# Pooled within-group SDs (used for SPSS-style standardisation)
sW <- sqrt(diag(SW_cov))
names(sW) <- colnames(X)

# ------------------------------------------------------------
# 4) Fit LDA on the ORIGINAL (unstandardised) variables
# ------------------------------------------------------------

fit <- MASS::lda(
  type ~ length + lwidth + rwidth + bottom + top + diagonal,
  data = banknotes
)

# Unstandardised canonical coefficients (Function 1 only)
b <- drop(fit$scaling[, 1])
names(b) <- colnames(X)

# ------------------------------------------------------------
# 5) SPSS "Standardized Canonical Discriminant Coefficients"
#    b* = b ⊙ sW
# ------------------------------------------------------------

b_spss <- b * sW

round(b_spss, 3)
```


```{r}
# In MASS::lda(), fit$svd are singular values from a whitened
# between-group matrix. Squaring them gives canonical roots up to
# a scaling convention.
lambda_mass <- fit$svd^2

# N = total number of observations, G = number of groups
G <- length(fit$lev)
df_w <- fit$N - G  # within-group degrees of freedom (N - G)

# To match the SPSS/book "Eigenvalue" column, adjust MASS's scaling by
# converting from an SSCP-style scaling to a covariance-style scaling.
# For this convention the conversion factor is (G - 1) / (N - G).
lambda_spss <- lambda_mass * (G - 1) / df_w
lambda_spss

# Canonical correlation corresponding to each eigenvalue:
# r^2 = lambda / (1 + lambda)
r <- sqrt(lambda_spss / (1 + lambda_spss))
r
```


#### Unstandardised
```{r}
fit <- MASS::lda(type ~ length + lwidth + rwidth + bottom + top + diagonal,
           data = banknotes)
fit
```

```{r}
lambda_mass <- fit$svd^2

G <- length(fit$lev)
df_w <- fit$N - G

lambda_spss <- lambda_mass * (G - 1) / df_w
lambda_spss

r <- sqrt(lambda_spss / (1 + lambda_spss))
r
```

### Soil type

```{r}
satellite <- read_sav("data/Book 3/satellite.sav") |>
  mutate(
    group = factor(
      group,
      levels = 1:6,
      labels = c(
        "red soil",
        "cotton crop",
        "grey soil",
        "damp grey soil",
        "stubble",
        "very damp grey soil"
      )
    )
  )
```

```{r}
X <- as.matrix(satellite[, c("bandA", "bandB", "bandC", "bandD")])
g <- as.factor(satellite$group)

p <- ncol(X)
n <- nrow(X)
groups <- levels(g)
G <- length(groups)

SW <- matrix(0, p, p)

for (grp in groups) {
  Xg <- X[g == grp, , drop = FALSE]
  Xg_centered <- scale(Xg, center = TRUE, scale = FALSE)
  SW <- SW + t(Xg_centered) %*% Xg_centered
}

SW_cov <- SW / (n - G)

sW <- sqrt(diag(SW_cov))
names(sW) <- colnames(X)

fit <- MASS::lda(group ~ bandA + bandB + bandC + bandD, data = satellite)

b <- fit$scaling                    # p x (<= min(p, G-1)) matrix
b_spss <- sweep(b, 1, sW, `*`)       # multiply each row by sW

if (b_spss[1] < 0) b_spss <- -b_spss

round(b_spss, 3)
```
```{r}
lambda_mass <- fit$svd^2

G <- length(fit$lev)
df_w <- fit$N - G

lambda_spss <- lambda_mass * (G - 1) / df_w
lambda_spss

r <- sqrt(lambda_spss / (1 + lambda_spss))
r
```

```{r}
# percent separation per discriminant function
pct_sep <- 100 * lambda_spss / sum(lambda_spss)

# cumulative percent separation
cum_pct_sep <- cumsum(pct_sep)

pct_sep
cum_pct_sep
```

```{r}
# Unstandardised coefficients (sign flipped to match SPSS)
-fit$scaling
```

## Extracting and plotting discriminant functions

### Banknotes
```{r}
fit <- MASS::lda(
  type ~ length + lwidth + rwidth + bottom + top + diagonal,
  data = banknotes
)
```

```{r}
scores <- predict(fit)$x  # matrix of LD scores (for 2 groups: one column, LD1)

banknotes_ld <- banknotes |> 
  mutate(LD1 = scores[, 1])
```

#### Histograms
```{r}
ggplot(banknotes_ld, aes(x = LD1)) +
  geom_histogram(bins = 30) +
  facet_wrap(~ type, ncol = 1) +
  theme_minimal()
```

#### Centroids
```{r}
banknotes_ld |> 
  group_by(type) |>
  summarise(across(starts_with("LD"), mean))
```

### Soil type

```{r}
fit <- MASS::lda(group ~ bandA + bandB + bandC + bandD, data = satellite)
fit
```

```{r}
scores <- predict(fit)$x  # matrix of LD scores (for 2 groups: one column, LD1)

satellite_ld <- satellite |> 
  mutate(LD1 = -scores[, 1], # Changed sign to match SPSS
         LD2 = scores[, 2],
         LD3 = scores[, 3],
         LD4 = scores[, 4])
```

#### Histograms
```{r}
ggplot(satellite_ld, aes(x = LD1, fill = factor(group))) +
  geom_histogram(bins = 30) +
  facet_wrap(~ group, ncol = 1) +
  theme_minimal()

ggplot(satellite_ld, aes(x = LD2, fill = factor(group))) +
  geom_histogram(bins = 30) +
  facet_wrap(~ group, ncol = 1) +
  theme_minimal()
```
#### Centroids

```{r}
satellite_ld |> 
  group_by(group) |>
  summarise(across(starts_with("LD"), mean)) |> 
  arrange(LD1)
```


#### Scatterplot
```{r}
ggplot(satellite_ld, aes(x = -LD1, y = LD2, colour = factor(group))) +
  geom_point(alpha = 0.3) +
  theme_minimal()
```

# Allocation

## Banknotes

```{r}
fit <- MASS::lda(
  type ~ length + lwidth + rwidth + bottom + top + diagonal,
  data = banknotes
)
```

### Predict

```{r}
pred <- predict(fit)
scores <- pred$x
class <- pred$class # Based on posterior probability (Baysean)

banknotes_pred <- banknotes_ld |>
  mutate(
    allocated = case_when(
      LD1 > 0 ~ "Counterfeit",
      LD1 < 0 ~ "Genuine",
      TRUE    ~ NA_character_
    ),
    allocated = factor(allocated, levels = levels(type)),
    class = class
  )
```

```{r}
# Note: class equivalent to allocation rule as binary
banknotes_pred |> count(allocated, class)
```

```{r}
banknotes_pred |>
  count(allocated, name = "Frequency") |>
  mutate(
    Percent = 100 * Frequency / sum(Frequency),
    ValidPercent = Percent,  # same because no missing values
    CumulativePercent = cumsum(ValidPercent)
  )
```

### Confusion matrix
```{r}
tabyl(banknotes_pred, type, allocated) |>
  adorn_totals(c("row", "col")) |>
  adorn_percentages("row") |>
  adorn_pct_formatting(digits = 1, affix_sign = TRUE) |>
  adorn_ns(position = "front") |>
  adorn_title(
    row_name = "Truth",
    col_name = "Allocated",
    placement = "top"
  )
```

## Soil type
```{r}
fit <- MASS::lda(group ~ bandA + bandB + bandC + bandD, data = satellite)
fit
```

```{r}
centroids_soil <- satellite_ld |> 
  group_by(group) |>
  summarise(LD1 = mean(LD1), .groups = "drop") |> 
  arrange(LD1)
centroids_soil
```

**Cut points from centroids**
```{r}
cuts <- centroids_soil |>
  reframe(cut = (LD1 + lead(LD1)) / 2) |>
  filter(!is.na(cut))
cuts
```

**Baysean posteriors**
```{r}
pred <- predict(fit)
class <- pred$class # Based on posterior probability (Baysean)
```

### Predict
```{r}

# labels in the same LD1-sorted order as your centroids
labs <- centroids_soil |> pull(group)

satellite_pred <- satellite_ld |>
  mutate(
    allocated = cut(
      LD1,
      breaks = c(-Inf, cuts$cut, Inf),
      labels = labs,
      ordered_result = TRUE,
      right = TRUE
    ),
    allocated = factor(allocated, levels = levels(group)),
    class = pred$class
  )
```


```{r}
satellite_pred |>
  count(allocated, name = "Frequency") |>
  mutate(
    Percent = 100 * Frequency / sum(Frequency),
    ValidPercent = Percent,  # same because no missing values
    CumulativePercent = cumsum(ValidPercent)
  )
```

### Confusion matrices

#### LD1 cut points
```{r}
tabyl(satellite_pred, group, allocated) |>
  adorn_totals(c("row", "col")) |>
  adorn_percentages("row") |>
  adorn_pct_formatting(digits = 0, affix_sign = TRUE) |>
  adorn_ns(position = "front") |> 
  gt(rowname_col = "Truth")
```

```{r}
satellite_pred |>
  summarise(accuracy = mean(as.character(group) == as.character(allocated)))
```

#### Baysean posteriors
```{r}
tabyl(satellite_pred, group, class) |>
  adorn_totals(c("row", "col")) |>
  adorn_percentages("row") |>
  adorn_pct_formatting(digits = 0, affix_sign = TRUE) |>
  adorn_ns(position = "front") |> 
  gt(rowname_col = "Truth")
```

```{r}
satellite_pred |>
  summarise(accuracy = mean(as.character(group) == as.character(class)))
```


### Soil type test data
```{r}
testsat <- read_sav("data/Book 3/testsat.sav") |> 
    mutate(
    group = factor(
      group,
      levels = 1:6,
      labels = c(
        "red soil",
        "cotton crop",
        "grey soil",
        "damp grey soil",
        "stubble",
        "very damp grey soil"
      )
    )
  )
```

#### Predict
```{r}
pred <- predict(fit, newdata = testsat)

testsat_pred <- testsat |>
  mutate(
    LD1 = -pred$x[, 1],
    allocated = cut(
      LD1,
      breaks = c(-Inf, cuts$cut, Inf),
      labels = labs,
      ordered_result = TRUE,
      right = TRUE
    ),
    allocated = factor(allocated, levels = levels(group)),
    class = pred$class
  )
```

```{r}
testsat_pred |>
  count(allocated, name = "Frequency") |>
  mutate(
    Percent = 100 * Frequency / sum(Frequency),
    ValidPercent = Percent,  # same because no missing values
    CumulativePercent = cumsum(ValidPercent)
  )
```

### Confusion matrices

#### LD1 cut points
```{r}
tabyl(testsat_pred, group, allocated) |>
  adorn_totals(c("row", "col")) |>
  adorn_percentages("row") |>
  adorn_pct_formatting(digits = 0, affix_sign = TRUE) |>
  adorn_ns(position = "front") |> 
  gt(rowname_col = "Truth")
```

```{r}
testsat_pred |>
  summarise(accuracy = mean(as.character(group) == as.character(allocated)))
```

#### Baysean posteriors
```{r}
tabyl(testsat_pred, group, class) |>
  adorn_totals(c("row", "col")) |>
  adorn_percentages("row") |>
  adorn_pct_formatting(digits = 0, affix_sign = TRUE) |>
  adorn_ns(position = "front") |> 
  gt(rowname_col = "Truth")
```

```{r}
testsat_pred |>
  summarise(accuracy = mean(as.character(group) == as.character(class)))
```

```{r}
testsat2 <- read_sav("data/Book 3/testsat2.sav") |> 
    mutate(
    group = factor(
      group,
      levels = 1:6,
      labels = c(
        "red soil",
        "cotton crop",
        "grey soil",
        "damp grey soil",
        "stubble",
        "very damp grey soil"
      )
    ),
    allocated = factor(
      allocated,
      levels = 1:6,
      labels = c(
        "red soil",
        "cotton crop",
        "grey soil",
        "damp grey soil",
        "stubble",
        "very damp grey soil"
      )
    )
  )
```
